# -*- coding: utf-8 -*-
"""ML-Assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kIcB5p992CbcZlIAQDQ1c24xJPSsG_z8

#ML-Assignment-2: Machine Learning and Data Science

**By:**<br>**Abdalkarim Eiss 1200015**<br>
**Razi Atyani 1200028**

##Libraries
"""

import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.linear_model import Lasso, Ridge, LinearRegression,LassoCV
from sklearn.exceptions import ConvergenceWarning
from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score
from sklearn.model_selection import validation_curve

"""```
# This is formatted as code
```

##Data Preprocessing :

###Read the dataset:
"""

# data frame
df = pd.read_csv('cars.csv')
# explore the dataset
df.head(10)

#Info about data
df.info()

# Shape about data
df.shape

#describe data
df.describe()

# Count the values of each feature
for clm in ['car name','brand','country','price','engine_capacity','cylinder','horse_power','top_speed','seats']:
    print(f'Name: {clm} dtype: {df[clm].dtype}\n')
    print(f'{df[clm].value_counts()}\n')
    print(('-' * 80) + '\n\n')

#brand column visualization
df['brand'].value_counts().plot.barh(figsize=(10,20));

#country visualization
df['country'].value_counts().plot.barh(figsize=(4,4));

"""###Document Missing Values:"""

# The sum of missing values in each column
df.isnull().sum()

# The percent of missing values from all dataset
print(df.isnull().any(axis=1).sum())
print(100*df.isnull().any(axis=1).sum()/df.shape[0],'%')

#the records with missing values
df[df.isnull().any(axis=1)]

#the duplicated rows
df[df.duplicated()].sum()

# The number of empty records
print(f"Number of empty records = {df.isnull().all(axis=1).sum()}")
df[df.isnull().all(axis=1)]

#missing percentage
missing_percentage = (df.isnull().sum().sort_values(ascending = False)/len(df))*100
missing_percentage

"""###Handling the Missing Values:

"""

#imputation with mode
df['cylinder'] = df['cylinder'].fillna(df['cylinder'].mode()[0])
print(df['cylinder'].mode()[0])

# Some limits found by internet:
LIMIT_HOURSE_POWER = 1_500.0
LIMIT_KMH = 530.0
LIMIT_ENGINE_CAPACITY = 8.4
LIMIT_CYLINDER_NR = 16.0

# Add new column
def add_columns(df, clm, function, new_clms):
    new_data = df[clm].apply(function).apply(pd.Series)
    new_data.columns = new_clms
    df = pd.concat([df, new_data], axis=1)
    return df

#price conversion
def apply_price_adj(price):
    try:
        if not isinstance(price, str) or len(price) < 5 or ' ' not in price[:4]:
            return [-1, 'ERR', -1, -1]

        c = price[:3].strip()
        price_str = price[4:].replace(',', '').strip()  # Remove thousand separators and extra spaces

        p = float(price_str)

        # Default prices in Euros and Dollars
        pe, pd = p, p

        conversion_rates = {
            'AED': (0.24, 0.27),
            'KWD': (2.93, 3.28),
            'OMR': (2.32, 2.60),
            'BHD': (2.37, 2.65),
            'QAR': (0.27, 0.25),
            'SAR': (0.24, 0.27),
            'EGP': (0.018, 0.021)
        }

        if c in conversion_rates:
            pe, pd = [p * rate for rate in conversion_rates[c]]

        return [p, c, pe, pd]

    except (ValueError, IndexError) as e:
        return [-1, 'ERR', -1, -1]

#handling columns
df_upd = df
# adjust price
df_upd = add_columns(df_upd, 'price', apply_price_adj, ['price_country', 'price_currency', 'price_euro', 'price_dollar'])

# Drop rows with NaN values (where conversion failed)
df_upd = df_upd.dropna(subset=['price'])

# adjust engine_capacity
df_upd['engine_capacity_float'] = pd.to_numeric(df_upd['engine_capacity'], errors='coerce')
df_upd['engine_capacity_l'] = np.where(df_upd['engine_capacity_float'] <= LIMIT_ENGINE_CAPACITY, df_upd['engine_capacity_float'], -1)


# adjust cylinder
df_upd['cylinder_float'] = pd.to_numeric(df_upd['cylinder'], errors='coerce')
df_upd['cylinder_nr'] = np.where(df_upd['cylinder_float'] <= LIMIT_CYLINDER_NR, df_upd['cylinder_float'], -1)

# adjust horse_power
df_upd['horse_power_float'] = pd.to_numeric(df_upd['horse_power'], errors='coerce')
df_upd['horse_power_cv'] = np.where(df_upd['horse_power_float'] <= LIMIT_HOURSE_POWER, df_upd['horse_power_float'], -1)

# adjust top_speed
df_upd['top_speed_float'] = pd.to_numeric(df_upd['top_speed'], errors='coerce')
df_upd['top_speed_kmh'] = np.where(df_upd['top_speed_float'] <= LIMIT_KMH, df_upd['top_speed_float'], -1)

#adjust seats
if 'seats' in df_upd.columns:
    df_upd['seats'] = df_upd['seats'].astype(str).str.replace(r'[^0-9.]', '', regex=True)
    df_upd['seats'] = df_upd['seats'].replace(['N A', 'NA', 'na', 'NaN'], np.nan)
    df_upd['seats'] = pd.to_numeric(df_upd['seats'], errors='coerce')

# adjust country
mapping = {
    'uae':'United Arab Emirates',
    'ksa':'Saudi Arabia',
    'kuwait':'Kuwait',
    'qatar':'Qatar',
    'oman':'Oman',
    'bahrain': 'Bahrain',
    'egypt': 'Egypt'
}
df_upd = df_upd.replace(mapping)
df_upd.drop([	'horse_power','top_speed' ,'price','cylinder', 'price_currency','price_euro','price_country', 'engine_capacity_float', 'cylinder_float', 'horse_power_float', 'top_speed_float', 'engine_capacity'], axis=1, inplace=True)
df_upd

#removing Invalid Data
dataset = df_upd

cond_pc = dataset['price_dollar'] == -1
cond_cap = dataset['engine_capacity_l'] == -1
cond_cyl = dataset['cylinder_nr'] == -1
cond_pow = dataset['horse_power_cv'] == -1
cond_ts = dataset['top_speed_kmh'] == -1

dataset['seats'] = dataset['seats'].fillna(dataset['seats'].mode()[0])

dataset = dataset[~(cond_pc | cond_cap | cond_cyl | cond_pow | cond_ts)]
dataset.reset_index(drop=True, inplace=True)
dataset.head(10)

#shape of the data after cleaning
dataset.shape

#Null values
df=dataset
df.isnull().sum()

#some visualisations about features

new_stat_clm = [ 'engine_capacity_l', 'country','cylinder_nr']

n_cols = 2
n_rows = (len(new_stat_clm) + n_cols - 1) // n_cols

fig, axes = plt.subplots(n_rows, n_cols)

axes = axes.flatten()

for ax, clm in zip(axes, new_stat_clm):
    value_counts = df[clm].value_counts()

    cmap = sns.color_palette("Spectral", as_cmap=True)
    colors = [cmap(i) for i in np.linspace(0, 1, len(value_counts))]

    value_counts.plot.bar(ax=ax, color=colors, figsize=(12, 18))

    ax.set_title(f'{clm}')
    ax.set_ylabel('')
    ax.tick_params(axis='x', rotation=45)

    for p in ax.patches:
        ax.annotate(
            f'{int(p.get_height())}',
            (p.get_x() + p.get_width() / 2., p.get_height()),
            ha='center',
            va='center',
            xytext=(0, 9),
            textcoords='offset points',
            fontsize=10
        )

for ax in axes[len(new_stat_clm):]:
    ax.remove()

plt.tight_layout()
plt.show()

#corroleation with target for features
brands = df['price_dollar'].unique()
f_num = ['engine_capacity_l', 'cylinder_nr','horse_power_cv','top_speed_kmh']
df_corr = df[f_num]
sns.heatmap(df_corr.corr(), annot=True, cmap='coolwarm', fmt='.2f', square=True);

#info about data after cleaning
df.info()

# Remove Outliers
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
columns_to_exclude = ['seats','price_dollar']
numeric_cols_to_process = [col for col in numeric_cols if col not in columns_to_exclude]

# Function to remove outliers based on IQR for a single column
def remove_outliers(df, column):
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    num_records_before = df.shape[0]

    print(f"{column}: Lower Bound = {lower_bound}, and Upper Bound = {upper_bound}")
    df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

    dropped_records = num_records_before - df.shape[0]
    print(f"Number of outliers removed from {column}: {dropped_records} ({100*dropped_records/num_records_before}%)")

    return df

df_no_outliers = df.copy()

for col in numeric_cols_to_process:
    df_no_outliers = remove_outliers(df_no_outliers, col)

print(f"Shape before removing outliers: {df.shape[0]}")
print(f"Shape after removing outliers: {df_no_outliers.shape[0]}")

n_cols = 3
n_rows = (len(numeric_cols_to_process) + n_cols - 1) // n_cols
fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 5))
axes = axes.flatten()

for i, col in enumerate(numeric_cols_to_process):
    sns.boxplot(x=df_no_outliers[col], ax=axes[i])
    axes[i].set_title(f'Boxplot of {col} (No Outliers)')

for j in range(i + 1, len(axes)):
    axes[j].axis('off')

plt.tight_layout()
plt.show()

df_no_outliers

#unique values for seats column
df_no_outliers["seats"].value_counts()

#reset index
df_no_outliers.reset_index(drop=True, inplace=True)
df_no_outliers

"""###Feature Engineering"""

#The LabelEncoder
label_encoder_brand = LabelEncoder()
label_encoder_country = LabelEncoder()
# Encode the 'brand' and 'country' columns
df_no_outliers['brand_encoded'] = label_encoder_brand.fit_transform(df_no_outliers['brand'])
df_no_outliers['country_encoded'] = label_encoder_country.fit_transform(df_no_outliers['country'])
df_labelenc = df_no_outliers.drop(['brand', 'country'], axis=1)
# Display the resulting DataFrame with encoded features
df_ohenc = df_labelenc
df_ohenc

#descrption of data
df_ohenc.describe()

#Scaling
df = df_ohenc.copy()
df_without_car_name = df.drop(columns=['car name'])
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_without_car_name)
scaled_df = pd.DataFrame(scaled_data, columns=df_without_car_name.columns)
print("Original Data (excluding 'car name'):")
print(df_without_car_name)
print("\nScaled Data:")
print(scaled_df)

#convert to float type
scaled_df=scaled_df.astype(float)

"""###Splitting the dataset:"""

#To split the dataset into train (0.6), test(0.2) and validation(0.2) using a function
train, test = train_test_split(scaled_df, test_size=0.2, random_state=42)
train, val = train_test_split(train, test_size=0.25, random_state=42)
print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print(f"val shape: {val.shape}")

"""##Building Regression Models:

### Drop target:
"""

### Split the data and the target in training and testing datasets
X_train = train.drop(columns=['price_dollar'])
y_train = train['price_dollar']


X_val = val.drop(columns=['price_dollar'])
y_val = val['price_dollar']


X_test = test.drop(columns=['price_dollar'])
y_test = test['price_dollar']

#make cpoies of training and testing ad valdation
X_train_original = X_train.copy()
X_val_original = X_val.copy()
X_test_original = X_test.copy()
y_train_original = y_train.copy()
y_val_original = y_val.copy()
y_test_original = y_test.copy()

"""###Linear Models:

####Linear regression closed form and Gradient descent
"""

# Add a column of ones for the intercept term
def add_intercept(X):
    return np.hstack((np.ones((X.shape[0], 1)), X))

# Closed-form solution
def closed_form_solution(X, y):
    X_b = add_intercept(X)
    theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
    return theta

# Gradient descent implementation with loss tracking for both training and validation
def gradient_descent_with_loss(X_train, y_train, X_val, y_val, lr=0.01, epochs=1000):
    X_train_b = add_intercept(X_train)
    X_val_b = add_intercept(X_val)
    m_train = len(y_train)

    theta = np.zeros(X_train_b.shape[1])
    train_losses = []
    val_losses = []

    for _ in range(epochs):
        gradients = (1 / m_train) * X_train_b.T @ (X_train_b @ theta - y_train)
        theta -= lr * gradients

        train_loss = mse(y_train, X_train_b @ theta)
        train_losses.append(train_loss)

        val_loss = mse(y_val, X_val_b @ theta)
        val_losses.append(val_loss)

    return theta, train_losses, val_losses

# Predictions
def predict(X, theta):
    X_b = add_intercept(X)
    return X_b @ theta

# Mean Squared Error
def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# Mean Absolute Error
def mae(y_true, y_pred):
    return np.mean(np.abs(y_true - y_pred))

# R-Squared Calculation
def r_squared(y_true, y_pred):
    ss_total = np.sum((y_true - np.mean(y_true)) ** 2)
    ss_residual = np.sum((y_true - y_pred) ** 2)
    return 1 - (ss_residual / ss_total)

# Prepare the data
X_train_np, y_train_np = X_train.values, y_train.values
X_val_np, y_val_np = X_val.values, y_val.values

# Apply closed-form solution
theta_closed = closed_form_solution(X_train_np, y_train_np)
predictions_closed = predict(X_val_np, theta_closed)

# Apply gradient descent with loss tracking
theta_gd, train_losses, val_losses = gradient_descent_with_loss(
    X_train_np, y_train_np, X_val_np, y_val_np, lr=0.01, epochs=1000
)
predictions_gd = predict(X_val_np, theta_gd)

# Evaluate models
mse_closed = mse(y_val_np, predictions_closed)
mse_gd = mse(y_val_np, predictions_gd)
r2_closed = r_squared(y_val_np, predictions_closed)
r2_gd = r_squared(y_val_np, predictions_gd)
mae_closed = mae(y_val_np, predictions_closed)
mae_gd = mae(y_val_np, predictions_gd)

# Print results
print("Closed-form solution parameters:", theta_closed)
print("Gradient descent parameters:", theta_gd)
print("MSE (Closed-form):", mse_closed)
print("MSE (Gradient Descent):", mse_gd)
print("MAE (Closed-form):", mae_closed)
print("MAE (Gradient Descent):", mae_gd)
print("R-squared (Closed-form):", r2_closed)
print("R-squared (Gradient Descent):", r2_gd)

# Visualization: True vs Predicted Values
plt.figure(figsize=(12, 6))
plt.scatter(y_val_np, predictions_closed, label='Closed-form Predictions', alpha=0.7, color='blue')
plt.scatter(y_val_np, predictions_gd, label='Gradient Descent Predictions', alpha=0.7, color='green')
plt.plot([min(y_val_np), max(y_val_np)], [min(y_val_np), max(y_val_np)], 'r--', label='Ideal Fit')
plt.xlabel("True Values (y_val)")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values")
plt.legend()
plt.grid(True)
plt.show()

# Visualization: Learning Curve (Training and Validation Losses)
plt.figure(figsize=(12, 6))
plt.plot(range(len(train_losses)), train_losses, label='Training Loss (MSE)', color='blue')
plt.plot(range(len(val_losses)), val_losses, label='Validation Loss (MSE)', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Mean Squared Error (MSE)")
plt.title("Learning Curve for Gradient Descent (Training vs. Validation)")
plt.legend()
plt.grid(True)
plt.show()

# Residual Plot for Closed-form and Gradient Descent Models
plt.figure(figsize=(12, 6))
plt.scatter(predictions_closed, y_val_np - predictions_closed, label='Closed-form Residuals', alpha=0.7, color='blue')
plt.scatter(predictions_gd, y_val_np - predictions_gd, label='Gradient Descent Residuals', alpha=0.7, color='green')
plt.axhline(y=0, color='red', linestyle='--', label='Zero Residual Line')
plt.xlabel("Predictions")
plt.ylabel("Residuals (True - Predicted)")
plt.title("Residual Plot (Closed-form vs Gradient Descent)")
plt.legend()
plt.grid(True)
plt.show()

# Error distribution histogram for both models
errors_closed = y_val_np - predictions_closed
errors_gd = y_val_np - predictions_gd

plt.figure(figsize=(12, 6))
plt.hist(errors_closed, bins=30, alpha=0.5, label='Closed-form Errors', color='blue')
plt.hist(errors_gd, bins=30, alpha=0.5, label='Gradient Descent Errors', color='green')
plt.xlabel("Prediction Error (True - Predicted)")
plt.ylabel("Frequency")
plt.title("Error Distribution (Closed-form vs Gradient Descent)")
plt.legend()
plt.grid(True)
plt.show()

"""####Lasso and Ridge Regularization"""

# Suppress specific warnings (e.g., ConvergenceWarning)
warnings.filterwarnings('ignore', category=ConvergenceWarning)

# Prepare the data
X_train_np, y_train_np = X_train.values, y_train.values
X_val_np, y_val_np = X_val.values, y_val.values

# Define a function for evaluating models and printing metrics
def evaluate_model(model, X_val, y_val, model_name="Model"):
    predictions = model.predict(X_val)
    mse_val = mean_squared_error(y_val, predictions)
    mae_val = mean_absolute_error(y_val, predictions)
    r2_val = r2_score(y_val, predictions)

    print(f"{model_name} Performance on Validation Set:")
    print(f"  MSE: {mse_val:.4f}")
    print(f"  MAE: {mae_val:.4f}")
    print(f"  R-squared: {r2_val:.4f}")

    return mse_val, mae_val, r2_val

# Apply Grid Search for Ridge Regression
ridge_model = Ridge()
ridge_params = {'alpha': np.logspace(-6, 6, 13)}
ridge_grid_search = GridSearchCV(ridge_model, ridge_params, cv=5, scoring='neg_mean_squared_error')
ridge_grid_search.fit(X_train_np, y_train_np)

# Best Ridge Model and its performance
best_ridge_model = ridge_grid_search.best_estimator_
print(f"Best Ridge Model (λ={ridge_grid_search.best_params_['alpha']})")
ridge_mse, ridge_mae, ridge_r2 = evaluate_model(best_ridge_model, X_val_np, y_val_np, "Ridge")

# Apply Grid Search for Lasso Regression
lasso_model = Lasso()
lasso_params = {'alpha': np.logspace(-6, 6, 13)}  # Trying different values of alpha (λ)
lasso_grid_search = GridSearchCV(lasso_model, lasso_params, cv=5, scoring='neg_mean_squared_error')
lasso_grid_search.fit(X_train_np, y_train_np)

# Best Lasso Model and its performance
best_lasso_model = lasso_grid_search.best_estimator_
print(f"Best Lasso Model (λ={lasso_grid_search.best_params_['alpha']})")
lasso_mse, lasso_mae, lasso_r2 = evaluate_model(best_lasso_model, X_val_np, y_val_np, "Lasso")

# Compare Models' Performance
print("\nComparison of Models:")
print(f"Ridge MSE: {ridge_mse:.4f}, MAE: {ridge_mae:.4f}, R-squared: {ridge_r2:.4f}")
print(f"Lasso MSE: {lasso_mse:.4f}, MAE: {lasso_mae:.4f}, R-squared: {lasso_r2:.4f}")

# Plotting the performance for different values of λ
# Plot Ridge Regularization Curve
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.plot(ridge_grid_search.cv_results_['param_alpha'], -ridge_grid_search.cv_results_['mean_test_score'], label="Ridge")
plt.xscale('log')
plt.xlabel('Lambda (α)')
plt.ylabel('Mean Test Score (Negative MSE)')
plt.title('Ridge: Grid Search for Optimal λ')
plt.grid(True)

# Plot Lasso Regularization Curve
plt.subplot(1, 2, 2)
plt.plot(lasso_grid_search.cv_results_['param_alpha'], -lasso_grid_search.cv_results_['mean_test_score'], label="Lasso", color='orange')
plt.xscale('log')
plt.xlabel('Lambda (α)')
plt.ylabel('Mean Test Score (Negative MSE)')
plt.title('Lasso: Grid Search for Optimal λ')
plt.grid(True)

plt.tight_layout()
plt.show()

# Predicted vs Actual values
plt.figure(figsize=(7, 7))
plt.scatter(y_val_np, best_ridge_model.predict(X_val_np), color='blue', label='Ridge', alpha=0.6)
plt.scatter(y_val_np, best_lasso_model.predict(X_val_np), color='orange', label='Lasso', alpha=0.6)
plt.plot([y_val_np.min(), y_val_np.max()], [y_val_np.min(), y_val_np.max()], color='black', linestyle='--', label='Ideal Fit')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('Predicted vs Actual Values')
plt.legend()
plt.grid(True)
plt.show()

# Residuals plot for Ridge
plt.figure(figsize=(7, 7))
plt.scatter(best_ridge_model.predict(X_val_np), best_ridge_model.predict(X_val_np) - y_val_np, color='blue', label='Ridge', alpha=0.6)
plt.scatter(best_lasso_model.predict(X_val_np), best_lasso_model.predict(X_val_np) - y_val_np, color='orange', label='Lasso', alpha=0.6)
plt.hlines(y=0, xmin=y_val_np.min(), xmax=y_val_np.max(), color='black', linestyle='--')
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals Plot')
plt.legend()
plt.grid(True)
plt.show()

# Plotting coefficients for Ridge and Lasso
plt.figure(figsize=(14, 6))

# Ridge Coefficients
plt.subplot(1, 2, 1)
plt.barh(range(len(best_ridge_model.coef_)), best_ridge_model.coef_, color='blue')
plt.yticks(range(len(best_ridge_model.coef_)), X_train.columns)
plt.title('Ridge Coefficients')
plt.xlabel('Coefficient Value')

# Lasso Coefficients
plt.subplot(1, 2, 2)
plt.barh(range(len(best_lasso_model.coef_)), best_lasso_model.coef_, color='orange')
plt.yticks(range(len(best_lasso_model.coef_)), X_train.columns)
plt.title('Lasso Coefficients')
plt.xlabel('Coefficient Value')

plt.tight_layout()
plt.show()

# Ridge Validation Curve
param_range = np.logspace(-6, 6, 13)
train_scores_ridge, test_scores_ridge = validation_curve(
    Ridge(), X_train_np, y_train_np, param_name="alpha", param_range=param_range, cv=5, scoring='neg_mean_squared_error')

# Lasso Validation Curve
train_scores_lasso, test_scores_lasso = validation_curve(
    Lasso(), X_train_np, y_train_np, param_name="alpha", param_range=param_range, cv=5, scoring='neg_mean_squared_error')

plt.figure(figsize=(14, 6))

# Ridge Validation Curve
plt.subplot(1, 2, 1)
plt.plot(param_range, -train_scores_ridge.mean(axis=1), label="Train Error (Ridge)", color='blue')
plt.plot(param_range, -test_scores_ridge.mean(axis=1), label="Validation Error (Ridge)", color='green')
plt.xscale('log')
plt.xlabel('Lambda (α)')
plt.ylabel('Negative MSE')
plt.title('Ridge Validation Curve')
plt.legend()
plt.grid(True)

# Lasso Validation Curve
plt.subplot(1, 2, 2)
plt.plot(param_range, -train_scores_lasso.mean(axis=1), label="Train Error (Lasso)", color='orange')
plt.plot(param_range, -test_scores_lasso.mean(axis=1), label="Validation Error (Lasso)", color='red')
plt.xscale('log')
plt.xlabel('Lambda (α)')
plt.ylabel('Negative MSE')
plt.title('Lasso Validation Curve')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""###Nonlinear Models:

####Polynomial Regression
"""

warnings.filterwarnings("ignore")  # Suppress all warnings
# Ensure X_train and y_train have the same number of samples
if len(X_train) != len(y_train):
    min_samples = min(len(X_train), len(y_train))
    X_train = X_train[:min_samples]
    y_train = y_train[:min_samples]

# Ensure X_val and y_val have the same number of samples
if len(X_val) != len(y_val):
    min_samples = min(len(X_val), len(y_val))
    X_val = X_val[:min_samples]
    y_val = y_val[:min_samples]

# Reshape X_train and X_val if 1D
if len(X_train.shape) == 1:
    X_train = X_train.reshape(-1, 1)
if len(X_val.shape) == 1:
    X_val = X_val.reshape(-1, 1)

# Define the range of polynomial degrees
degrees = range(2, 11)

# Containers for storing metrics
r2_train_values = {}
r2_val_values = {}
mse_train_values = {}
mse_val_values = {}
mae_train_values = {}
mae_val_values = {}

# Models and transformations
models = {}

# --- Train Models and Store Metrics ---
for degree in degrees:
    polynomial_features = PolynomialFeatures(degree, include_bias=True)
    X_train_poly = polynomial_features.fit_transform(X_train)
    X_val_poly = polynomial_features.transform(X_val)

    # Train the model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    models[degree] = (model, polynomial_features)

    # Evaluate on training set
    y_train_pred = model.predict(X_train_poly)
    r2_train_values[degree] = r2_score(y_train, y_train_pred)
    mse_train_values[degree] = mean_squared_error(y_train, y_train_pred)
    mae_train_values[degree] = mean_absolute_error(y_train, y_train_pred)

    # Evaluate on validation set
    y_val_pred = model.predict(X_val_poly)
    r2_val_values[degree] = r2_score(y_val, y_val_pred)
    mse_val_values[degree] = mean_squared_error(y_val, y_val_pred)
    mae_val_values[degree] = mean_absolute_error(y_val, y_val_pred)

# --- Print Metrics ---
print("Metrics for Training and Validation Sets:")
print(f"{'Degree':<10}{'MSE Train':<15}{'MSE Val':<15}{'MAE Train':<15}{'MAE Val':<15}{'R^2 Train':<15}{'R^2 Val':<15}")
for degree in degrees:
    print(f"{degree:<10}{mse_train_values[degree]:<15.4f}{mse_val_values[degree]:<15.4f}"
          f"{mae_train_values[degree]:<15.4f}{mae_val_values[degree]:<15.4f}"
          f"{r2_train_values[degree]:<15.4f}{r2_val_values[degree]:<15.4f}")

# --- Visualizations ---
fig, axes = plt.subplots(3, 2, figsize=(14, 15))

# MSE
axes[0, 0].bar(mse_train_values.keys(), mse_train_values.values(), color='blue', alpha=0.7)
axes[0, 0].set_title('Training Mean Squared Error (MSE)', fontsize=14)
axes[0, 0].set_xlabel('Polynomial Degree', fontsize=12)
axes[0, 0].set_ylabel('MSE', fontsize=12)

axes[0, 1].bar(mse_val_values.keys(), mse_val_values.values(), color='orange', alpha=0.7)
axes[0, 1].set_title('Validation Mean Squared Error (MSE)', fontsize=14)
axes[0, 1].set_xlabel('Polynomial Degree', fontsize=12)
axes[0, 1].set_ylabel('MSE', fontsize=12)
# MAE
axes[1, 0].bar(mae_train_values.keys(), mae_train_values.values(), color='green', alpha=0.7)
axes[1, 0].set_title('Training Mean Absolute Error (MAE)', fontsize=14)
axes[1, 0].set_xlabel('Polynomial Degree', fontsize=12)
axes[1, 0].set_ylabel('MAE', fontsize=12)

axes[1, 1].bar(mae_val_values.keys(), mae_val_values.values(), color='red', alpha=0.7)
axes[1, 1].set_title('Validation Mean Absolute Error (MAE)', fontsize=14)
axes[1, 1].set_xlabel('Polynomial Degree', fontsize=12)
axes[1, 1].set_ylabel('MAE', fontsize=12)
# R²
axes[2, 0].bar(r2_train_values.keys(), r2_train_values.values(), color='purple', alpha=0.7)
axes[2, 0].set_title('Training R² Score', fontsize=14)
axes[2, 0].set_xlabel('Polynomial Degree', fontsize=12)
axes[2, 0].set_ylabel('R² Score', fontsize=12)
axes[2, 0].set_ylim(0, 1)

axes[2, 1].bar(r2_val_values.keys(), r2_val_values.values(), color='cyan', alpha=0.7)
axes[2, 1].set_title('Validation R² Score', fontsize=14)
axes[2, 1].set_xlabel('Polynomial Degree', fontsize=12)
axes[2, 1].set_ylabel('R² Score', fontsize=12)
axes[2, 1].set_ylim(0, 1)

plt.tight_layout()
plt.show()

# ---  True vs Predicted Values ---
plt.figure(figsize=(10, 6))
for degree in degrees:
    model, poly = models[degree]
    X_train_poly = poly.transform(X_train)
    y_pred = model.predict(X_train_poly)
    plt.scatter(y_train, y_pred, label=f"Degree {degree}", alpha=0.7)
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2, label="Perfect Fit")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values for Polynomial Models")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# ---  Residuals vs Predicted Values ---
plt.figure(figsize=(10, 6))
for degree in degrees:
    model, poly = models[degree]
    X_train_poly = poly.transform(X_train)
    y_pred = model.predict(X_train_poly)
    residuals = y_train - y_pred
    plt.scatter(y_pred, residuals, label=f"Degree {degree}", alpha=0.7)
plt.axhline(y=0, color='k', linestyle='--', linewidth=1)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values for Polynomial Models")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

"""####Standard Gaussian kernel(RBF)."""

# Ensure X_train and y_train are consistent
if len(X_train) != len(y_train):
    min_samples = min(len(X_train), len(y_train))
    X_train = X_train[:min_samples]
    y_train = y_train[:min_samples]

# Reshape X_train if 1D
if len(X_train.shape) == 1:
    X_train = X_train.reshape(-1, 1)

X_train = np.array(X_train, dtype=float)
y_train = np.array(y_train, dtype=float)

# Check for empty dataset
if X_train.size == 0 or y_train.size == 0:
    raise ValueError("Input data (X_train or y_train) is empty!")

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Define Gaussian kernel function
def gaussian_kernel(x, center, sigma):
    return np.exp(-np.linalg.norm(x - center) ** 2 / (2 * sigma ** 2))

# Construct the RBF feature matrix
def rbf_feature_matrix(X, centers, sigma):
    return np.array([[gaussian_kernel(x, c, sigma) for c in centers] for x in X])

# Define ranges for centers and sigma
num_centers_range = [50, 100, 150, 200]
sigma_values = [1.0, 2.0, 3.0, 4.0, 5.0]

# Containers for metrics
results = []

# Train models and evaluate
for num_centers in num_centers_range:
    centers = np.linspace(X_train.min(), X_train.max(), num_centers).reshape(-1, 1)
    for sigma in sigma_values:
        # Construct the feature matrix
        Phi_train = rbf_feature_matrix(X_train, centers, sigma)
        Phi_val = rbf_feature_matrix(X_val, centers, sigma)

        # Solve for weights using least squares
        weights = np.linalg.lstsq(Phi_train, y_train, rcond=None)[0]

        # Predictions
        y_train_pred = Phi_train @ weights
        y_val_pred = Phi_val @ weights

        # Evaluate metrics
        train_mae = mean_absolute_error(y_train, y_train_pred)
        val_mae = mean_absolute_error(y_val, y_val_pred)
        train_mse = mean_squared_error(y_train, y_train_pred)
        val_mse = mean_squared_error(y_val, y_val_pred)
        train_r2 = r2_score(y_train, y_train_pred)
        val_r2 = r2_score(y_val, y_val_pred)

        # Store results
        results.append({
            "num_centers": num_centers,
            "sigma": sigma,
            "train_mae": train_mae,
            "val_mae": val_mae,
            "train_mse": train_mse,
            "val_mse": val_mse,
            "train_r2": train_r2,
            "val_r2": val_r2
        })

# Identify the best model (based on lowest validation MAE)
best_model = min(results, key=lambda x: x["val_mae"])

print(f"Best Model: Centers = {best_model['num_centers']}, Sigma = {best_model['sigma']}")
print(f"Validation MAE = {best_model['val_mae']:.4f}, Validation MSE = {best_model['val_mse']:.4f}, Validation R² = {best_model['val_r2']:.4f}")

# --- Validation True vs Predicted Values ---
Phi_val = rbf_feature_matrix(X_val, np.linspace(X_train.min(), X_train.max(), best_model["num_centers"]).reshape(-1, 1), best_model["sigma"])

y_val_pred = Phi_val @ weights  # This will work if Phi_val.shape[1] == weights.shape[0]


plt.figure(figsize=(10, 6))
plt.scatter(y_val, y_val_pred, alpha=0.7, color='blue', label="Predictions")
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'k--', lw=2, label="Perfect Fit")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --- Validation Residuals ---
residuals = y_val - y_val_pred
plt.figure(figsize=(10, 6))
plt.scatter(y_val_pred, residuals, alpha=0.7, color='red', label="Residuals")
plt.axhline(y=0, color='k', linestyle='--', linewidth=1)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values ")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --- MAE and MSE Comparison ---
fig, axes = plt.subplots(2, 1, figsize=(10, 10))

# Validation MAE
val_mae = [result["val_mae"] for result in results if result["num_centers"] == best_model["num_centers"]]
axes[0].bar(sigma_values, val_mae, color='blue', alpha=0.7)
axes[0].set_title(f'Validation MAE for Sigma Values (Centers = {best_model["num_centers"]})', fontsize=14)
axes[0].set_xlabel('Sigma', fontsize=12)
axes[0].set_ylabel('MAE', fontsize=12)

# Validation MSE
val_mse = [result["val_mse"] for result in results if result["num_centers"] == best_model["num_centers"]]
axes[1].bar(sigma_values, val_mse, color='orange', alpha=0.7)
axes[1].set_title(f'Validation MSE for Sigma Values (Centers = {best_model["num_centers"]})', fontsize=14)
axes[1].set_xlabel('Sigma', fontsize=12)
axes[1].set_ylabel('MSE', fontsize=12)

plt.tight_layout()
plt.show()

"""####Non-linearModels-comparision"""

import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

def rbf_transform(X, lambd=1.0):
    centers = np.mean(X, axis=0)
    return np.exp(-np.square(X - centers) / lambd)

# Ensure X_train and y_train have the same number of samples
if len(X_train) != len(y_train):
    min_samples = min(len(X_train), len(y_train))
    X_train = X_train[:min_samples]
    y_train = y_train[:min_samples]

# Ensure X_val and y_val have the same number of samples
if len(X_val) != len(y_val):
    min_samples = min(len(X_val), len(y_val))
    X_val = X_val[:min_samples]
    y_val = y_val[:min_samples]

# Reshape X_train and X_val if 1D
if len(X_train.shape) == 1:
    X_train = X_train.reshape(-1, 1)
if len(X_val.shape) == 1:
    X_val = X_val.reshape(-1, 1)

# Define the range of polynomial degrees
degrees = range(2, 11)

# Containers for storing metrics
r2_train_values = {}
r2_val_values = {}
mse_train_values = {}
mse_val_values = {}
mae_train_values = {}
mae_val_values = {}

# Models and transformations
models = {}
rbf_metrics = {}  # Store RBF metrics

# --- Train Polynomial Models ---
for degree in degrees:
    polynomial_features = PolynomialFeatures(degree, include_bias=True)
    X_train_poly = polynomial_features.fit_transform(X_train)
    X_val_poly = polynomial_features.transform(X_val)

    # Train the model
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    models[f"poly_{degree}"] = (model, polynomial_features)

    # Evaluate on training set
    y_train_pred = model.predict(X_train_poly)
    r2_train_values[f"poly_{degree}"] = r2_score(y_train, y_train_pred)
    mse_train_values[f"poly_{degree}"] = mean_squared_error(y_train, y_train_pred)
    mae_train_values[f"poly_{degree}"] = mean_absolute_error(y_train, y_train_pred)

    # Evaluate on validation set
    y_val_pred = model.predict(X_val_poly)
    r2_val_values[f"poly_{degree}"] = r2_score(y_val, y_val_pred)
    mse_val_values[f"poly_{degree}"] = mean_squared_error(y_val, y_val_pred)
    mae_val_values[f"poly_{degree}"] = mean_absolute_error(y_val, y_val_pred)

# --- Train RBF Model ---
# Evaluate on validation set
    y_val_pred = model.predict(X_val_poly)
    r2_val_values[f"poly_{degree}"] = r2_score(y_val, y_val_pred)
    mse_val_values[f"poly_{degree}"] = mean_squared_error(y_val, y_val_pred)
    mae_val_values[f"poly_{degree}"] = mean_absolute_error(y_val, y_val_pred)

# --- Train RBF Model ---
lambd_values = [0.1, 0.5, 1.0, 2.0,3.0,4.0,5.0]  # ,Different lambda values for RBF
for lambd in lambd_values:
    X_train_rbf = rbf_transform(X_train, lambd)
    X_val_rbf = rbf_transform(X_val, lambd)

    # Train the model
    model = LinearRegression()
    model.fit(X_train_rbf, y_train)
    models[f"rbf_{lambd}"] = model

    # Evaluate on training set
    y_train_pred = model.predict(X_train_rbf)
    r2_train_values[f"rbf_{lambd}"] = r2_score(y_train, y_train_pred)
    mse_train_values[f"rbf_{lambd}"] = mean_squared_error(y_train, y_train_pred)
    mae_train_values[f"rbf_{lambd}"] = mean_absolute_error(y_train, y_train_pred)

    # Evaluate on validation set
    y_val_pred = model.predict(X_val_rbf)
    r2_val_values[f"rbf_{lambd}"] = r2_score(y_val, y_val_pred)
    mse_val_values[f"rbf_{lambd}"] = mean_squared_error(y_val, y_val_pred)
    mae_val_values[f"rbf_{lambd}"] = mean_absolute_error(y_val, y_val_pred)

# --- Print Metrics ---
print("Metrics for Training and Validation Sets:")
print(f"{'Transformation':<15}{'Param':<10}{'MSE Train':<15}{'MSE Val':<15}{'MAE Train':<15}{'MAE Val':<15}{'R^2 Train':<15}{'R^2 Val':<15}")
for key in mse_train_values:
    transform, param = key.split('_')
    print(f"{transform:<15}{param:<10}{mse_train_values[key]:<15.4f}{mse_val_values[key]:<15.4f}"
          f"{mae_train_values[key]:<15.4f}{mae_val_values[key]:<15.4f}"
          f"{r2_train_values[key]:<15.4f}{r2_val_values[key]:<15.4f}")

# --- Comparison Plots ---
plt.figure(figsize=(10, 6))
for key in models.keys():
    if "rbf" in key:
        lambd = float(key.split('_')[1])
        X_train_transformed = rbf_transform(X_train, lambd)
    else:  # Polynomial
        degree = int(key.split('_')[1])
        _, poly = models[key]
        X_train_transformed = poly.transform(X_train)

    model = models[key][0] if "poly" in key else models[key]
    y_pred = model.predict(X_train_transformed)
    plt.scatter(y_train, y_pred, label=key, alpha=0.7)

plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'k--', lw=2, label="Perfect Fit")
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values (Polynomial vs. RBF)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

"""##The Best Model Tuning and Evaluation

####Feature Selection with Forward Selection
"""

import math

n = 8  # Number of features
d = 3  # Polynomial degree

total_features = math.comb(n + d, d)
print(f"Total number of features: {total_features}")

# --- Feature Selection with Polynomial Features and Forward Selection ---
poly = PolynomialFeatures(degree=3, include_bias=False)

# Fit PolynomialFeatures to extract transformed feature names
poly.fit(X_train_original)
transformed_feature_names = poly.get_feature_names_out(input_features=X_train_original.columns)
selected_features = []
remaining_features = list(transformed_feature_names)
best_r2 = -float('inf')

while remaining_features:
    r2_with_features = {}

    for feature in remaining_features:
        features_to_test = selected_features + [feature]
        selected_indices = [list(transformed_feature_names).index(f) for f in features_to_test]
        X_train_poly = poly.fit_transform(X_train_original)[:, selected_indices]
        X_val_poly = poly.transform(X_val_original)[:, selected_indices]

        # Train the model and calculate R²
        model = LinearRegression()
        model.fit(X_train_poly, y_train_original)
        predictions = model.predict(X_val_poly)
        r2 = r2_score(y_val_original, predictions)
        r2_with_features[feature] = r2

    # Select the feature that provides the highest R²
    best_feature = max(r2_with_features, key=r2_with_features.get)
    if r2_with_features[best_feature] > best_r2:
        best_r2 = r2_with_features[best_feature]
        selected_features.append(best_feature)
        remaining_features.remove(best_feature)
    else:
        break  # Stop if no improvement in R²

# Output the selected features
print("Selected Features:", selected_features)
print("Best R² Score on Validation Set (Before Tuning):", best_r2)
E=len(selected_features)
print("The number of selected feature",E)

"""####Applying Regularization Techniques with grid search

We apply both and see which better here with the model
"""

# --- Hyperparameter Tuning with Ridge Regression ---
print("Performing  Ridge hyperparameter tuning...")
selected_indices = [list(transformed_feature_names).index(f) for f in selected_features]
X_train_poly = poly.fit_transform(X_train_original)[:, selected_indices]
X_val_poly = poly.transform(X_val_original)[:, selected_indices]

param_grid = {'alpha': [0.1, 1, 10, 100]}  # Grid for Ridge regularization strength
ridge = Ridge()
grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train_poly, y_train_original)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print("Best  Ridge Hyperparameters:", best_params)

# --- Evaluate on Validation Set (After Tuning) ---
val_predictions = best_model.predict(X_val_poly)
mse_val = mean_squared_error(y_val_original, val_predictions)
mae_val = mean_absolute_error(y_val_original, val_predictions)
r2_val = r2_score(y_val_original, val_predictions)

print("Validation Metrics After  Ridge Tuning:")
print(f"MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R²: {r2_val:.4f}")

# --- Feature Scaling ---
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_original)
X_val_scaled = scaler.transform(X_val_original)
X_test_scaled = scaler.transform(X_test_original)

# --- LASSO Regularization ---
print("Performing LASSO Regression with ")
selected_indices = [list(transformed_feature_names).index(f) for f in selected_features]
X_train_poly = poly.transform(X_train_scaled)[:, selected_indices]
X_val_poly = poly.transform(X_val_scaled)[:, selected_indices]

# Train LASSO with Cross-Validation
lasso = LassoCV(cv=5, random_state=42)
lasso.fit(X_train_poly, y_train_original)

# Evaluate LASSO on validation set
lasso_predictions_val = lasso.predict(X_val_poly)
lasso_r2_val = r2_score(y_val_original, lasso_predictions_val)
lasso_mse_val = mean_squared_error(y_val_original, lasso_predictions_val)
lasso_mae_val = mean_absolute_error(y_val_original, lasso_predictions_val)
lasso_alpha = lasso.alpha_

print(f"LASSO Optimal Alpha: {lasso_alpha}")
print(f"LASSO Validation R²: {lasso_r2_val:.4f}, MSE: {lasso_mse_val:.4f},MAE: {lasso_mae_val:.4f}")

"""####Evaluation"""

# --- Evaluate on Test Set ---
print("Evaluating on test set...")
X_test_poly = poly.transform(X_test_original)[:, selected_indices]
test_predictions = best_model.predict(X_test_poly)

mse_test = mean_squared_error(y_test_original, test_predictions)
mae_test = mean_absolute_error(y_test_original, test_predictions)
r2_test = r2_score(y_test_original, test_predictions)

print("Test Set Metrics:")
print(f"MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R²: {r2_test:.4f}")

# --- Visualizations ---
# True vs Predicted Values
plt.figure(figsize=(10, 6))
plt.scatter(y_test_original, test_predictions, alpha=0.7, color='blue', label='Predicted vs True')
plt.plot([y_test_original.min(), y_test_original.max()], [y_test_original.min(), y_test_original.max()], 'k--', lw=2, label='Perfect Fit')
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values on Test Set")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Residuals vs Predicted Values
residuals = y_test_original - test_predictions
plt.figure(figsize=(10, 6))
plt.scatter(test_predictions, residuals, alpha=0.7, color='red')
plt.axhline(y=0, color='k', linestyle='--', linewidth=1)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values on Test Set")
plt.grid(alpha=0.3)
plt.show()

"""##Optional

We choose horse_power as we looked to the corraltion and it has been corralted to most features

splitting:
"""

#To split the dataset into train (0.6), test(0.2) and validation(0.2) using a function
train, test = train_test_split(scaled_df, test_size=0.2, random_state=42)
train, val = train_test_split(train, test_size=0.25, random_state=42)
print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print(f"val shape: {val.shape}")
#dropping
X_train = train.drop(columns=['horse_power_cv'])
y_train = train['horse_power_cv']
X_val = val.drop(columns=['horse_power_cv'])
y_val = val['horse_power_cv']
X_test = test.drop(columns=['horse_power_cv'])
y_test = test['horse_power_cv']

"""Now we trained it on the best model"""

# Fit PolynomialFeatures
poly = PolynomialFeatures(degree=3, include_bias=False)
poly.fit(X_train)  # Fit on the training data
transformed_feature_names = poly.get_feature_names_out(input_features=X_train.columns)

# Initialize variables
selected_features = []
remaining_features = list(transformed_feature_names)
best_r2 = -float('inf')

# Iteratively select features
while remaining_features:
    r2_with_features = {}

    for feature in remaining_features:
        features_to_test = selected_features + [feature]
        selected_indices = [list(transformed_feature_names).index(f) for f in features_to_test]

        # Transform data with selected features
        X_train_poly = poly.fit_transform(X_train)[:, selected_indices]
        X_val_poly = poly.transform(X_val)[:, selected_indices]

        # Train the model and calculate R²
        model = LinearRegression()
        model.fit(X_train_poly, y_train)
        predictions = model.predict(X_val_poly)
        r2 = r2_score(y_val, predictions)
        r2_with_features[feature] = r2

    # Select the feature with the highest R² improvement
    best_feature = max(r2_with_features, key=r2_with_features.get)
    if r2_with_features[best_feature] > best_r2:
        best_r2 = r2_with_features[best_feature]
        selected_features.append(best_feature)
        remaining_features.remove(best_feature)
    else:
        break  # Stop if no improvement in R²

# Output results
print("Selected Features:", selected_features)
print("Best R² Score on Validation Set (Before Tuning):", best_r2)
E = len(selected_features)
print("The number of selected features:", E)

# --- Hyperparameter Tuning with Ridge Regression ---
print("Performing  Ridge hyperparameter tuning...")
selected_indices = [list(transformed_feature_names).index(f) for f in selected_features]
X_train_poly = poly.fit_transform(X_train)[:, selected_indices]
X_val_poly = poly.transform(X_val)[:, selected_indices]

param_grid = {'alpha': [0.1, 1, 10, 100]}  # Grid for Ridge regularization strength
ridge = Ridge()
grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)
grid_search.fit(X_train_poly, y_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print("Best  Ridge Hyperparameters:", best_params)

# --- Evaluate on Validation Set (After Tuning) ---
val_predictions = best_model.predict(X_val_poly)
mse_val = mean_squared_error(y_val, val_predictions)
mae_val = mean_absolute_error(y_val, val_predictions)
r2_val = r2_score(y_val, val_predictions)

print("Validation Metrics After  Ridge Tuning:")
print(f"MSE: {mse_val:.4f}, MAE: {mae_val:.4f}, R²: {r2_val:.4f}")
# --- Evaluate on Test Set ---
print("Evaluating on test set...")
X_test_poly = poly.transform(X_test)[:, selected_indices]
test_predictions = best_model.predict(X_test_poly)

mse_test = mean_squared_error(y_test, test_predictions)
mae_test = mean_absolute_error(y_test, test_predictions)
r2_test = r2_score(y_test, test_predictions)

print("Test Set Metrics:")
print(f"MSE: {mse_test:.4f}, MAE: {mae_test:.4f}, R²: {r2_test:.4f}")

# --- Visualizations ---
# True vs Predicted Values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, test_predictions, alpha=0.7, color='blue', label='Predicted vs True')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2, label='Perfect Fit')
plt.xlabel("True Values")
plt.ylabel("Predicted Values")
plt.title("True vs. Predicted Values on Test Set")
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# Residuals vs Predicted Values
residuals = y_test_original - test_predictions
plt.figure(figsize=(10, 6))
plt.scatter(test_predictions, residuals, alpha=0.7, color='red')
plt.axhline(y=0, color='k', linestyle='--', linewidth=1)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values on Test Set")
plt.grid(alpha=0.3)
plt.show()